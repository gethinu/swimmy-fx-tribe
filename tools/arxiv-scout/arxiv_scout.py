#!/usr/bin/env python3
"""
arXiv Scout v2 - è«–æ–‡è‡ªå‹•å·¡å›ãƒ»é€šçŸ¥ã‚·ã‚¹ãƒ†ãƒ 
ç‹¬ç«‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

æ©Ÿèƒ½:
- arXiv APIã§é–¢é€£è«–æ–‡ã‚’6æ™‚é–“ã”ã¨ã«å·¡å›
- Gemini APIã§10æ®µéšè©•ä¾¡ï¼ˆå…·ä½“çš„ãªæ´»ç”¨æ³•ã‚’å‡ºåŠ›ï¼‰
- æ¯æœ8æ™‚ã«Discordã¸ã‚µãƒãƒªãƒ¼é€šçŸ¥
- HIGHè©•ä¾¡è«–æ–‡ã‚’æ°¸ç¶šã‚¹ãƒˆãƒƒã‚¯
- ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°/AIéƒ¨é–€ã®2ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
"""

import os
import hashlib
import sys
import json
import time
import logging
import requests
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from pathlib import Path

# =============================================================================
# ãƒ­ã‚°è¨­å®š
# =============================================================================

PROJECT_DIR = Path(__file__).parent
LOG_DIR = PROJECT_DIR / "logs"
LOG_DIR.mkdir(exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] %(levelname)s: %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[
        logging.FileHandler(LOG_DIR / "arxiv_scout.log", encoding="utf-8"),
        logging.StreamHandler(sys.stdout),
    ],
)
logger = logging.getLogger(__name__)

# =============================================================================
# è¨­å®š
# =============================================================================

PROJECT_DIR = Path(__file__).parent
DATA_DIR = PROJECT_DIR / "data"
DATA_DIR.mkdir(exist_ok=True)

# Discord Webhook URL (from env)
def _strip_inline_comment(value: str) -> str:
    if not value:
        return value
    for sep in (" #", "\t#"):
        if sep in value:
            return value.split(sep, 1)[0].rstrip()
    return value.rstrip()


def resolve_discord_webhook():
    raw = os.getenv("SWIMMY_ARXIV_REPORT_WEBHOOK", "")
    raw = raw.strip().strip('"').strip("'")
    return _strip_inline_comment(raw)


def mask_webhook(url: str) -> str:
    if not url:
        return "unset"
    tail = url[-6:] if len(url) > 6 else url
    return f"...{tail}"


# Gemini API - .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
def load_env_file():
    """Swimmyã®.envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã‚€"""
    env_paths = [
        Path("/home/swimmy/swimmy/config/.env"),
        Path("/home/swimmy/swimmy/.env"),
        Path("/home/swimmy/.env"),
    ]
    for env_path in env_paths:
        if not env_path.exists():
            continue
        with open(env_path, "r") as f:
            for line in f:
                line = line.strip()
                if line.startswith("export ") and "=" in line:
                    line = line[7:]  # 'export ' ã‚’å‰Šé™¤
                if "=" in line and not line.startswith("#"):
                    key, value = line.split("=", 1)
                    value = value.strip('"').strip("'")
                    os.environ[key] = value
        return


load_env_file()
GEMINI_API_KEY = os.getenv("SWIMMY_GEMINI_API_KEY")
GEMINI_AVAILABLE = bool(GEMINI_API_KEY)
_GEMINI_WARNED = False

# =============================================================================
# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åŸºæº–ï¼ˆæ¤œç´¢ã‚¯ã‚¨ãƒªï¼‰
# =============================================================================
# arXivã¯å…¨è«–æ–‡ã‚’å–å¾—ã™ã‚‹APIã¯ãªã„ã€‚ã‚«ãƒ†ã‚´ãƒª/ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ã™ã‚‹ä»•çµ„ã¿ã€‚
# ä»¥ä¸‹ã®ã‚¯ã‚¨ãƒªã§é–¢é€£åˆ†é‡ã‚’ç¶²ç¾…çš„ã«ã‚«ãƒãƒ¼ã€‚

SEARCH_QUERIES = [
    # ===== é‡‘èãƒ»ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° =====
    "cat:q-fin.TR",  # Quantitative Finance - Trading & Market Microstructure
    "cat:q-fin.PM",  # Portfolio Management
    "cat:q-fin.RM",  # Risk Management
    "cat:q-fin.CP",  # Computational Finance
    "cat:q-fin.ST",  # Statistical Finance
    # ===== å¼·åŒ–å­¦ç¿’ Ã— é‡‘è =====
    "all:reinforcement+learning+trading",
    "all:reinforcement+learning+portfolio",
    "all:deep+reinforcement+learning+finance",
    # ===== æ™‚ç³»åˆ—äºˆæ¸¬ =====
    "all:time+series+forecasting+transformer",
    "all:stock+prediction+deep+learning",
    "all:forex+prediction",
    # ===== LLMãƒ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ =====
    "all:large+language+model+finance",
    "all:LLM+agent+autonomous",
    "all:multi+agent+trading",
    # ===== å› æœæ¨è«–ãƒ»æ„æ€æ±ºå®š =====
    "all:causal+inference+decision+making",
]

# =============================================================================
# RePEc RSS (NEP) - lightweight intake
# =============================================================================

REPEC_RSS_FEEDS = [
    ("https://nep.repec.org/rss/nep-fmk.rss.xml", ["q-fin.TR"]),
    ("https://nep.repec.org/rss/nep-mon.rss.xml", ["q-fin.RM"]),
    ("https://nep.repec.org/rss/nep-rmg.rss.xml", ["q-fin.RM"]),
    ("https://nep.repec.org/rss/nep-big.rss.xml", ["cs.LG"]),
    ("https://nep.repec.org/rss/nep-cmp.rss.xml", ["cs.LG"]),
    ("https://nep.repec.org/rss/nep-ict.rss.xml", ["cs.LG"]),
]

REPEC_KEYWORDS_FX = [
    "fx",
    "forex",
    "foreign exchange",
    "currency",
    "exchange rate",
    "market microstructure",
    "trading",
]
REPEC_KEYWORDS_AI = [
    "ai",
    "artificial intelligence",
    "machine learning",
    "deep learning",
    "transformer",
    "llm",
    "reinforcement learning",
    "neural",
]


def _env_int(name: str, default: int) -> int:
    try:
        return int(os.getenv(name, default))
    except Exception:
        return default


# Increase to avoid missing new papers beyond top-10 results per query
MAX_RESULTS_PER_QUERY = _env_int("SWIMMY_ARXIV_MAX_RESULTS", 30)

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
SEEN_PAPERS_FILE = DATA_DIR / "seen_papers.json"
PENDING_PAPERS_FILE = DATA_DIR / "pending_papers.json"
STOCK_PAPERS_FILE = DATA_DIR / "stock_papers.json"  # HIGHè©•ä¾¡è«–æ–‡ã®æ°¸ç¶šä¿å­˜
LAST_REPORT_FILE = DATA_DIR / "last_report.json"  # Discordé€£æºç”¨ï¼ˆç•ªå·â†’è«–æ–‡ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰

# =============================================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =============================================================================


def load_json(filepath):
    if filepath.exists():
        with open(filepath, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def save_json(filepath, data):
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)


# =============================================================================
# arXiv API
# =============================================================================


def search_arxiv(query, max_results=20):
    """arXiv APIã§æ¤œç´¢"""
    base_url = "http://export.arxiv.org/api/query"
    params = {
        "search_query": query,
        "start": 0,
        "max_results": max_results,
        "sortBy": "submittedDate",
        "sortOrder": "descending",
    }

    try:
        response = requests.get(base_url, params=params, timeout=30)
        response.raise_for_status()
        return response.text
    except Exception as e:
        logger.info(f" API Error: {e}")
        return None


def parse_arxiv_response(xml_text):
    """arXiv APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹"""
    papers = []
    try:
        root = ET.fromstring(xml_text)
        ns = {"atom": "http://www.w3.org/2005/Atom"}

        for entry in root.findall("atom:entry", ns):
            paper = {
                "id": entry.find("atom:id", ns).text.split("/")[-1],
                "title": entry.find("atom:title", ns).text.strip().replace("\n", " "),
                "summary": entry.find("atom:summary", ns).text.strip(),  # å…¨æ–‡å–å¾—
                "authors": [
                    a.find("atom:name", ns).text
                    for a in entry.findall("atom:author", ns)
                ][:5],
                "published": entry.find("atom:published", ns).text[:10],
                "link": entry.find("atom:id", ns).text,
            }
            categories = entry.findall("atom:category", ns)
            paper["categories"] = [c.get("term") for c in categories][:5]
            papers.append(paper)
    except Exception as e:
        logger.info(f" Parse Error: {e}")
    return papers


# =============================================================================
# RePEc RSS helpers
# =============================================================================


def fetch_repec_rss(feed_url: str):
    try:
        resp = requests.get(feed_url, timeout=20)
        resp.raise_for_status()
        return resp.text
    except Exception as e:
        logger.info(f" RePEc RSS Error: {e}")
        return None


def parse_repec_rss(xml_text: str):
    items = []
    if not xml_text:
        return items
    try:
        root = ET.fromstring(xml_text)
        ns = {
            "rss": "http://purl.org/rss/1.0/",
            "rdf": "http://www.w3.org/1999/02/22-rdf-syntax-ns#",
            "dc": "http://purl.org/dc/elements/1.1/",
        }
        for item in root.findall("rss:item", ns):
            title = item.findtext("rss:title", default="", namespaces=ns)
            link = item.findtext("rss:link", default="", namespaces=ns)
            desc = item.findtext("rss:description", default="", namespaces=ns)
            creators = [c.text for c in item.findall("dc:creator", ns) if c.text]
            date = item.findtext("dc:date", default="", namespaces=ns)
            items.append(
                {
                    "title": title.strip(),
                    "link": link.strip(),
                    "description": (desc or "").strip(),
                    "authors": creators[:5],
                    "published": (date or "").strip(),
                }
            )
    except Exception as e:
        logger.info(f" RePEc RSS Parse Error: {e}")
    return items


def repec_matches_keywords(text: str, keywords):
    if not text:
        return False
    lower = text.lower()
    return any(kw in lower for kw in keywords)


def _extract_arxiv_id_from_repec_link(link: str):
    marker = "RePEc:arx:papers:"
    if marker in link:
        return link.split(marker, 1)[1].split("&", 1)[0]
    return None


def normalize_repec_item(item: dict, default_categories):
    link = item.get("link", "")
    arxiv_id = _extract_arxiv_id_from_repec_link(link)
    if arxiv_id:
        paper_id = arxiv_id
    else:
        base = (item.get("title", "") + link).encode("utf-8")
        paper_id = "repec:" + hashlib.sha1(base).hexdigest()
    return {
        "id": paper_id,
        "title": item.get("title", ""),
        "summary": item.get("description", ""),
        "authors": item.get("authors", [])[:5],
        "published": item.get("published", ""),
        "link": link,
        "categories": default_categories,
    }


def fetch_repec_papers():
    papers = []
    for feed_url, default_categories in REPEC_RSS_FEEDS:
        xml_text = fetch_repec_rss(feed_url)
        if not xml_text:
            continue
        items = parse_repec_rss(xml_text)
        keywords = (
            REPEC_KEYWORDS_FX
            if "q-fin" in " ".join(default_categories)
            else REPEC_KEYWORDS_AI
        )
        for item in items:
            text = f"{item.get('title', '')} {item.get('description', '')}"
            if not repec_matches_keywords(text, keywords):
                continue
            papers.append(normalize_repec_item(item, default_categories))
    return papers


# =============================================================================
# Gemini API è©•ä¾¡ï¼ˆè«–æ–‡å†…å®¹ã‚’æ·±ãèª­ã‚“ã§åˆ¤æ–­ï¼‰
# =============================================================================


def evaluate_with_gemini(paper):
    """Gemini APIã§è«–æ–‡å†…å®¹ã‚’æ·±ãèª­ã‚“ã§è©•ä¾¡"""
    global _GEMINI_WARNED
    if not GEMINI_AVAILABLE:
        if not _GEMINI_WARNED:
            logger.info(" âš ï¸ GEMINI_API_KEYæœªè¨­å®š: ç°¡æ˜“è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            _GEMINI_WARNED = True
        return None

    full_abstract = paper.get("summary", "")

    prompt = f"""ã‚ãªãŸã¯é‡‘èå·¥å­¦ã¨AIç ”ç©¶ã®å°‚é–€å®¶ã§ã‚ã‚Šã€FXãƒˆãƒ¬ãƒ¼ãƒ€ãƒ¼ã§ã‚‚ã‚ã‚Šã¾ã™ã€‚
ä»¥ä¸‹ã®è«–æ–‡ã‚’èª­ã‚“ã§ã€å®Ÿå‹™çš„ãªè¦³ç‚¹ã‹ã‚‰è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘{paper['title']}
ã€è‘—è€…ã€‘{', '.join(paper['authors'])}
ã€ã‚«ãƒ†ã‚´ãƒªã€‘{', '.join(paper['categories'])}
ã€Abstractã€‘
{full_abstract}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ä»¥ä¸‹ã®JSONå½¢å¼**ã®ã¿**ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜æ–‡ä¸è¦ï¼‰:

{{
  "score": 1ã€œ10ã®æ•´æ•°,
  "summary_ja": "è«–æ–‡ã®å†…å®¹ã‚’å¹³æ˜“ãªæ—¥æœ¬èªã§èª¬æ˜ï¼ˆ80å­—ç¨‹åº¦ï¼‰",
  "key_tech": "æ ¸ã¨ãªã‚‹æŠ€è¡“/æ‰‹æ³•åï¼ˆä¾‹: LSTM, Transformer, DQN, Kalman Filterç­‰ï¼‰",
  "how_to_use": "ã“ã®è«–æ–‡ã®çŸ¥è¦‹ã‚’FXãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«ã©ã†æ´»ã‹ã›ã‚‹ã‹ã€å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’èª¬æ˜ï¼ˆ100å­—ç¨‹åº¦ï¼‰ã€‚ä¾‹: ã€ã“ã®æ‰‹æ³•ã‚’ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚·ã‚°ãƒŠãƒ«ã«çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€‡ã€‡ã§ãã‚‹ã€ã€ã€‡ã€‡ã®åˆ¤æ–­ã«ä½¿ãˆã‚‹ã€ãªã©"
}}

ã€ã‚¹ã‚³ã‚¢åŸºæº–ã€‘
10=å³å®Ÿè£…å¯èƒ½ãªé©æ–°çš„æ‰‹æ³•, 9=å¼·åŒ–å­¦ç¿’Ã—é‡‘èã®æœ€å…ˆç«¯, 8=æ™‚ç³»åˆ—äºˆæ¸¬ã«ç›´æ¥ä½¿ãˆã‚‹
7=å·¥å¤«æ¬¡ç¬¬ã§ä½¿ãˆã‚‹, 6=æ”¹è‰¯ã ãŒå¿œç”¨ä½™åœ°ã‚ã‚Š, 5=é–“æ¥å‚è€ƒ
4=å¿œç”¨å›°é›£, 3=ç†è«–ã®ã¿, 2=ã»ã¼ç„¡é–¢ä¿‚, 1=ç„¡é–¢ä¿‚

ã€é‡è¦ã€‘
- how_to_useã¯ã€Œã€œã«å½¹ç«‹ã¤å¯èƒ½æ€§ãŒã‚ã‚‹ã€ã®ã‚ˆã†ãªæ›–æ˜§ãªè¡¨ç¾ã§ã¯ãªãã€å…·ä½“çš„ã«ã©ã†ã™ã‚‹ã‹ã‚’æ›¸ãã“ã¨
- ä¾‹: ã€Œãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ€¥å¤‰æ™‚ã®ãƒ­ãƒƒãƒˆèª¿æ•´ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«çµ„ã¿è¾¼ã‚ã‚‹ã€ã€Œãƒˆãƒ¬ãƒ³ãƒ‰è»¢æ›ã®æ¤œçŸ¥ç²¾åº¦ã‚’ä¸Šã’ã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã¨ã—ã¦ä½¿ãˆã‚‹ã€"""

    try:
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GEMINI_API_KEY}"
        response = requests.post(
            url,
            json={
                "contents": [{"parts": [{"text": prompt}]}],
                "generationConfig": {"temperature": 0.5, "maxOutputTokens": 600},
            },
            timeout=90,
        )

        if response.status_code == 200:
            result = response.json()
            text = result["candidates"][0]["content"]["parts"][0]["text"]
            start = text.find("{")
            end = text.rfind("}") + 1
            if start >= 0 and end > start:
                eval_data = json.loads(text[start:end])
                score = int(eval_data.get("score", 5))
                eval_data["score"] = score
                # ã‚¹ã‚³ã‚¢ã‹ã‚‰relevanceã‚’è‡ªå‹•è¨ˆç®—
                if score >= 8:
                    eval_data["relevance"] = "HIGH"
                elif score >= 5:
                    eval_data["relevance"] = "MEDIUM"
                else:
                    eval_data["relevance"] = "LOW"
                return eval_data
        else:
            logger.info(f" Gemini API Error: {response.status_code}")
    except Exception as e:
        logger.info(f" Gemini Error: {e}")

    return None


# =============================================================================
# Discord é€šçŸ¥ï¼ˆ2ã‚«ãƒ†ã‚´ãƒª: ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°/AIéƒ¨é–€ï¼‰
# =============================================================================

# ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ç”¨
TRADING_CATEGORIES = {
    "q-fin.TR",
    "q-fin.PM",
    "q-fin.RM",
    "q-fin.CP",
    "q-fin.ST",
    "q-fin.GN",
    "q-fin.MF",
}
AI_CATEGORIES = {"cs.LG", "cs.AI", "cs.NE", "cs.CL", "stat.ML"}


FALLBACK_TECH_KEYWORDS = [
    ("transformer", "Transformer"),
    ("attention", "Attention"),
    ("lstm", "LSTM"),
    ("gru", "GRU"),
    ("rnn", "RNN"),
    ("cnn", "CNN"),
    ("kalman", "Kalman Filter"),
    ("hmm", "HMM"),
    ("gpt", "GPT/LLM"),
    ("llm", "LLM"),
    ("reinforcement learning", "Reinforcement Learning"),
    ("policy gradient", "Policy Gradient"),
    ("q-learning", "Q-Learning"),
    ("xgboost", "XGBoost"),
    ("graph", "Graph/Network"),
]


def infer_key_tech(text):
    text = text.lower()
    for kw, label in FALLBACK_TECH_KEYWORDS:
        if kw in text:
            return label
    return "-"


def score_to_priority(score):
    try:
        s = int(score)
    except Exception:
        return "C"
    if s >= 9:
        return "S"
    if s >= 8:
        return "A"
    if s >= 6:
        return "B"
    return "C"


def fallback_evaluate(paper):
    """Geminiæœªä½¿ç”¨æ™‚ã®ç°¡æ˜“è©•ä¾¡"""
    cats = set(paper.get("categories", []))
    if cats & TRADING_CATEGORIES:
        score = 6
        how_to_use = "ã‚·ã‚°ãƒŠãƒ«ç”Ÿæˆ/ãƒªã‚¹ã‚¯ç®¡ç†ã®ä»®èª¬ã¨ã—ã¦æ¤œè¨ã€‚"
    elif cats & AI_CATEGORIES:
        score = 5
        how_to_use = "ç‰¹å¾´é‡è¨­è¨ˆã‚„ãƒ¢ãƒ‡ãƒ«é¸å®šã®å‚è€ƒã¨ã—ã¦æ¤œè¨ã€‚"
    else:
        score = 4
        how_to_use = "é–¢é€£åº¦ãŒä½ã„ãŸã‚å‚è€ƒç¨‹åº¦ã€‚"

    relevance = "HIGH" if score >= 8 else ("MEDIUM" if score >= 5 else "LOW")

    summary_src = (paper.get("summary") or "").replace("\n", " ").strip()
    if summary_src:
        summary_ja = summary_src[:120] + ("..." if len(summary_src) > 120 else "")
    else:
        summary_ja = (paper.get("title") or "")[:120]

    text_blob = f"{paper.get('title', '')} {paper.get('summary', '')}"
    key_tech = infer_key_tech(text_blob)

    return {
        "score": score,
        "summary_ja": summary_ja,
        "key_tech": key_tech,
        "how_to_use": how_to_use,
        "relevance": relevance,
    }

def classify_paper(paper):
    """è«–æ–‡ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°/AIéƒ¨é–€ã«åˆ†é¡"""
    cats = set(paper.get("categories", []))
    # é‡‘èç³»ã‚«ãƒ†ã‚´ãƒªãŒã‚ã‚Œã°ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    if cats & TRADING_CATEGORIES:
        return "trading"
    # AI/MLç³»ã‚«ãƒ†ã‚´ãƒªãŒã‚ã‚Œã°AI
    if cats & AI_CATEGORIES:
        return "ai"
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§åˆ¤å®š
    title_lower = paper.get("title", "").lower()
    if any(
        kw in title_lower
        for kw in ["trading", "forex", "portfolio", "stock", "financial", "market"]
    ):
        return "trading"
    return "ai"


def is_repec_paper(paper):
    link = (paper.get("link") or "").lower()
    pid = (paper.get("id") or "").lower()
    return ("repec.org" in link) or pid.startswith("repec:")


def send_summary_to_discord(papers_with_eval):
    """2ã‚«ãƒ†ã‚´ãƒªã«åˆ†ã‘ã¦Discordã«é€ä¿¡"""
    if not papers_with_eval:
        logger.info(" é€šçŸ¥ã™ã‚‹è«–æ–‡ãªã—")
        return
    webhook_url = resolve_discord_webhook()
    if not webhook_url:
        logger.info(" âš ï¸ Discord Webhookæœªè¨­å®š: é€šçŸ¥ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        return

    # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
    papers_with_eval.sort(key=lambda x: x.get("score", 0), reverse=True)

    # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
    trading_papers = []
    ai_papers = []
    repec_papers = []

    for p in papers_with_eval:
        cat = classify_paper(p)
        if cat == "trading":
            trading_papers.append(p)
        else:
            ai_papers.append(p)
        if is_repec_paper(p):
            repec_papers.append(p)

    now = datetime.now().strftime("%Y-%m-%d")

    # Pre-assign global numbers for Discordé€£æº (must match last_report.json)
    global_num = 1
    for p in trading_papers + ai_papers:
        p["number"] = global_num
        global_num += 1

    # ===== ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°éƒ¨é–€ =====
    if trading_papers:
        high_count = len([p for p in trading_papers if p.get("relevance") == "HIGH"])
        med_count = len([p for p in trading_papers if p.get("relevance") == "MEDIUM"])

        content = f"# ğŸ“ˆ arXiv Scout - ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°éƒ¨é–€\n"
        content += f"**{now}** | ğŸ”¥ HIGH: {high_count}ä»¶ | ğŸ“„ MEDIUM: {med_count}ä»¶\n"
        content += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

        for p in trading_papers[:5]:  # TOP 5
            num = p["number"]  # Use global number
            score = p.get("score", 0)
            emoji = "ğŸ”¥" if score >= 8 else ("ğŸ“„" if score >= 5 else "â­ï¸")
            priority = score_to_priority(score)

            content += (
                f"**{num}. [{priority}] {emoji} [{score}/10] {p['title'][:50]}**\n"
            )
            content += f"ğŸ”§ **æŠ€è¡“:** {p.get('key_tech', '-')}\n"
            content += f"ğŸ“ {p.get('summary_ja', '')}\n"
            content += (
                f"ğŸ’¡ **æ´»ç”¨æ³•:** {p.get('how_to_use', p.get('usefulness', ''))}\n"
            )
            content += f"ğŸ”— <{p['link']}>\n\n"

        if len(content) > 1950:
            content = content[:1900] + "\n\n...(ç¶šãã¯ã‚¹ãƒˆãƒƒã‚¯å‚ç…§)"

        try:
            resp = requests.post(webhook_url, json={"content": content}, timeout=15)
            logger.info(
                f" âœ… ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°éƒ¨é–€é€ä¿¡: {resp.status_code} ({mask_webhook(webhook_url)})"
            )
        except Exception as e:
            logger.info(f" âŒ Discord Error: {e}")

        time.sleep(2)

    # ===== AI/MLéƒ¨é–€ =====
    if ai_papers:
        high_count = len([p for p in ai_papers if p.get("relevance") == "HIGH"])
        med_count = len([p for p in ai_papers if p.get("relevance") == "MEDIUM"])

        content = f"# ğŸ¤– arXiv Scout - AI/MLéƒ¨é–€\n"
        content += f"**{now}** | ğŸ”¥ HIGH: {high_count}ä»¶ | ğŸ“„ MEDIUM: {med_count}ä»¶\n"
        content += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

        for p in ai_papers[:5]:  # TOP 5
            num = p["number"]  # Use global number
            score = p.get("score", 0)
            emoji = "ğŸ”¥" if score >= 8 else ("ğŸ“„" if score >= 5 else "â­ï¸")
            priority = score_to_priority(score)

            content += (
                f"**{num}. [{priority}] {emoji} [{score}/10] {p['title'][:50]}**\n"
            )
            content += f"ğŸ”§ **æŠ€è¡“:** {p.get('key_tech', '-')}\n"
            content += f"ğŸ“ {p.get('summary_ja', '')}\n"
            content += (
                f"ğŸ’¡ **æ´»ç”¨æ³•:** {p.get('how_to_use', p.get('usefulness', ''))}\n"
            )
            content += f"ğŸ”— <{p['link']}>\n\n"

        if len(content) > 1950:
            content = content[:1900] + "\n\n...(ç¶šãã¯ã‚¹ãƒˆãƒƒã‚¯å‚ç…§)"

        try:
            resp = requests.post(webhook_url, json={"content": content}, timeout=15)
            logger.info(
                f" âœ… AI/MLéƒ¨é–€é€ä¿¡: {resp.status_code} ({mask_webhook(webhook_url)})"
            )
        except Exception as e:
            logger.info(f" âŒ Discord Error: {e}")

    # ===== RePEc Picks =====
    repec_trading = [p for p in repec_papers if classify_paper(p) == "trading"]
    repec_ai = [p for p in repec_papers if classify_paper(p) != "trading"]

    def _send_repec_section(title, papers):
        high_count = len([p for p in papers if p.get("relevance") == "HIGH"])
        med_count = len([p for p in papers if p.get("relevance") == "MEDIUM"])

        content = f"# ğŸ“š RePEc Picks - {title}\n"
        content += f"**{now}** | ğŸ”¥ HIGH: {high_count}ä»¶ | ğŸ“„ MEDIUM: {med_count}ä»¶\n"
        content += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

        for p in papers[:5]:  # TOP 5 (empty ok)
            num = p["number"]
            score = p.get("score", 0)
            emoji = "ğŸ”¥" if score >= 8 else ("ğŸ“„" if score >= 5 else "â­ï¸")
            priority = score_to_priority(score)

            content += (
                f"**{num}. [{priority}] {emoji} [{score}/10] {p['title'][:50]}**\n"
            )
            content += f"ğŸ”§ **æŠ€è¡“:** {p.get('key_tech', '-')}\n"
            content += f"ğŸ“ {p.get('summary_ja', '')}\n"
            content += (
                f"ğŸ’¡ **æ´»ç”¨æ³•:** {p.get('how_to_use', p.get('usefulness', ''))}\n"
            )
            content += f"ğŸ”— <{p['link']}>\n\n"

        if len(content) > 1950:
            content = content[:1900] + "\n\n...(ç¶šãã¯ã‚¹ãƒˆãƒƒã‚¯å‚ç…§)"

        try:
            resp = requests.post(webhook_url, json={"content": content}, timeout=15)
            logger.info(
                f" âœ… RePEc Picksé€ä¿¡: {resp.status_code} ({mask_webhook(webhook_url)})"
            )
        except Exception as e:
            logger.info(f" âŒ Discord Error: {e}")

    _send_repec_section("ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°", repec_trading)
    _send_repec_section("AI/ML", repec_ai)

    # ===== Discordé€£æºç”¨: ç•ªå·â†’è«–æ–‡ãƒãƒƒãƒ”ãƒ³ã‚°ä¿å­˜ =====
    all_papers = trading_papers + ai_papers
    report_data = {
        "date": now,
        "papers": [],
        "trading_count": len(trading_papers),
        "ai_count": len(ai_papers),
    }
    for i, p in enumerate(all_papers[:20], 1):  # æœ€å¤§20ä»¶
        priority = score_to_priority(p.get("score", 0))
        report_data["papers"].append(
            {
                "number": i,
                "id": p.get("id"),
                "title": p.get("title"),
                "link": p.get("link"),
                "score": p.get("score"),
                "priority": priority,
                "category": classify_paper(p),
                "key_tech": p.get("key_tech"),
                "how_to_use": p.get("how_to_use", p.get("usefulness", "")),
            }
        )
    save_json(LAST_REPORT_FILE, report_data)
    logger.info(f" ğŸ“ last_report.jsonä¿å­˜: {len(report_data['papers'])}ä»¶")


# =============================================================================
# HIGHè«–æ–‡ã‚¹ãƒˆãƒƒã‚¯
# =============================================================================


def stock_high_papers(papers_with_eval):
    """HIGHè©•ä¾¡ã®è«–æ–‡ã‚’æ°¸ç¶šä¿å­˜"""
    stock_data = load_json(STOCK_PAPERS_FILE)
    stock_papers = stock_data.get("papers", [])
    stock_ids = {p["id"] for p in stock_papers}

    added = 0
    for p in papers_with_eval:
        if p.get("relevance") == "HIGH" and p["id"] not in stock_ids:
            stock_papers.append(
                {
                    "id": p["id"],
                    "title": p["title"],
                    "link": p["link"],
                    "summary_ja": p.get("summary_ja", ""),
                    "usefulness": p.get("usefulness", ""),
                    "key_tech": p.get("key_tech", ""),
                    "score": p.get("score", 0),
                    "published": p["published"],
                    "stocked_at": datetime.now().isoformat(),
                }
            )
            added += 1

    stock_data["papers"] = stock_papers
    stock_data["last_updated"] = datetime.now().isoformat()
    save_json(STOCK_PAPERS_FILE, stock_data)

    logger.info(f" ğŸ“¦ {added}ä»¶ã®HIGHè«–æ–‡ã‚’ã‚¹ãƒˆãƒƒã‚¯")
    return added


# =============================================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =============================================================================


def scout_papers():
    """è«–æ–‡å·¡å›ï¼‹è©•ä¾¡"""
    print(f"\n[Scout] ğŸ” å·¡å›é–‹å§‹: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    seen_data = load_json(SEEN_PAPERS_FILE)
    seen_ids = set(seen_data.get("papers", []))

    pending_data = load_json(PENDING_PAPERS_FILE)
    pending_papers = pending_data.get("papers", [])

    new_count = 0

    for query in SEARCH_QUERIES:
        logger.info(f" æ¤œç´¢: {query[:40]}...")

        xml_response = search_arxiv(query, max_results=MAX_RESULTS_PER_QUERY)
        if not xml_response:
            continue

        papers = parse_arxiv_response(xml_response)

        for paper in papers:
            paper_id = paper["id"]
            if paper_id in seen_ids:
                continue

            logger.info(f" ğŸ“„ {paper['title'][:50]}...")

            # Geminiè©•ä¾¡ï¼ˆå¤±æ•—æ™‚ã¯ç°¡æ˜“è©•ä¾¡ï¼‰
            evaluation = evaluate_with_gemini(paper)
            if not evaluation:
                evaluation = fallback_evaluate(paper)

            paper_with_eval = {**paper, **evaluation}

            # HIGH/MEDIUMã®ã¿ä¿å­˜
            if evaluation.get("relevance") in ["HIGH", "MEDIUM"]:
                pending_papers.append(paper_with_eval)
                new_count += 1
                print(
                    f"[Scout] âœ… {evaluation.get('relevance')} (score {evaluation.get('score')})"
                )
            else:
                logger.info(" â­ï¸ LOW - ã‚¹ã‚­ãƒƒãƒ—")

            seen_ids.add(paper_id)
            time.sleep(0.5)  # APIåˆ¶é™å¯¾ç­–

        time.sleep(1)

    # RePEc RSS (optional, lightweight)
    repec_papers = fetch_repec_papers()
    for paper in repec_papers:
        paper_id = paper["id"]
        if paper_id in seen_ids:
            continue

        evaluation = evaluate_with_gemini(paper)
        if not evaluation:
            evaluation = fallback_evaluate(paper)

        paper_with_eval = {**paper, **evaluation}

        if evaluation.get("relevance") in ["HIGH", "MEDIUM"]:
            pending_papers.append(paper_with_eval)
            new_count += 1
        seen_ids.add(paper_id)

    # ä¿å­˜
    seen_data["papers"] = list(seen_ids)[-3000:]
    seen_data["last_check"] = datetime.now().isoformat()
    save_json(SEEN_PAPERS_FILE, seen_data)

    pending_data["papers"] = pending_papers
    pending_data["last_updated"] = datetime.now().isoformat()
    save_json(PENDING_PAPERS_FILE, pending_data)

    logger.info(f" âœ… å·¡å›å®Œäº†: {new_count}ä»¶ã‚’è©•ä¾¡ãƒ»ä¿å­˜")
    return new_count


def send_morning_report():
    """æœ8æ™‚ã®ã‚µãƒãƒªãƒ¼é€šçŸ¥"""
    print(f"\n[Scout] ğŸ“¬ æœã®ãƒ¬ãƒãƒ¼ãƒˆ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    today = datetime.now().date()
    last_sent_date = get_last_sent_date()
    if last_sent_date == today:
        logger.info(" ğŸ” æœ¬æ—¥åˆ†ã¯æ—¢ã«é€ä¿¡æ¸ˆã¿")
        return

    pending_data = load_json(PENDING_PAPERS_FILE)
    pending_papers = pending_data.get("papers", [])

    if pending_papers:
        # HIGHè«–æ–‡ã‚’ã‚¹ãƒˆãƒƒã‚¯
        stock_high_papers(pending_papers)

        # ã‚µãƒãƒªãƒ¼é€ä¿¡
        send_summary_to_discord(pending_papers)

        # ãƒšãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚¯ãƒªã‚¢
        pending_data["papers"] = []
        pending_data["last_sent"] = datetime.now().isoformat()
        save_json(PENDING_PAPERS_FILE, pending_data)
    else:
        logger.info(" ãƒšãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãªã—")
        pending_data["last_sent"] = datetime.now().isoformat()
        save_json(PENDING_PAPERS_FILE, pending_data)


def get_last_sent_date():
    data = load_json(PENDING_PAPERS_FILE)
    last_sent = data.get("last_sent")
    if not last_sent:
        return None
    try:
        return datetime.fromisoformat(last_sent).date()
    except Exception:
        return None


def run_scheduler():
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ãƒ¢ãƒ¼ãƒ‰"""
    logger.info(" ğŸš€ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼é–‹å§‹")
    logger.info(" ğŸ“… å·¡å›: 0:00, 6:00, 12:00, 18:00")
    logger.info(" ğŸ“¬ é€šçŸ¥: æ¯æœ 8:00")

    # åˆå›å·¡å›
    scout_papers()

    # 08:00ã‚’éãã¦èµ·å‹•ã—ãŸå ´åˆã®ã‚­ãƒ£ãƒƒãƒã‚¢ãƒƒãƒ—é€ä¿¡
    now = datetime.now()
    last_sent_date = get_last_sent_date()
    if now.hour >= 8 and last_sent_date != now.date():
        logger.info(" â±ï¸ 08:00ã‚’éãã¦èµ·å‹•: æœãƒ¬ãƒãƒ¼ãƒˆã‚’ã‚­ãƒ£ãƒƒãƒã‚¢ãƒƒãƒ—é€ä¿¡")
        send_morning_report()

    while True:
        now = datetime.now()

        # æ¬¡ã®ã‚¤ãƒ™ãƒ³ãƒˆè¨ˆç®—
        current_hour = now.hour
        next_scout_hour = ((current_hour // 6) + 1) * 6
        if next_scout_hour >= 24:
            next_scout = now.replace(hour=0, minute=0, second=0) + timedelta(days=1)
        else:
            next_scout = now.replace(hour=next_scout_hour, minute=0, second=0)

        if now.hour < 8:
            next_notify = now.replace(hour=8, minute=0, second=0)
        else:
            next_notify = now.replace(hour=8, minute=0, second=0) + timedelta(days=1)

        if next_scout < next_notify:
            wait_seconds = (next_scout - now).total_seconds()
            event = "scout"
            next_time = next_scout
        else:
            wait_seconds = (next_notify - now).total_seconds()
            event = "notify"
            next_time = next_notify

        print(
            f"[Scout] ğŸ’¤ æ¬¡å› {event}: {next_time.strftime('%m/%d %H:%M')} ({wait_seconds/3600:.1f}hå¾Œ)"
        )
        time.sleep(max(wait_seconds, 60))

        if event == "scout":
            scout_papers()
        else:
            send_morning_report()


def show_stock():
    """ã‚¹ãƒˆãƒƒã‚¯è«–æ–‡ã‚’è¡¨ç¤º"""
    stock_data = load_json(STOCK_PAPERS_FILE)
    papers = stock_data.get("papers", [])

    print(f"\nğŸ“¦ HIGHè«–æ–‡ã‚¹ãƒˆãƒƒã‚¯ ({len(papers)}ä»¶)\n")
    for i, p in enumerate(papers[-10:], 1):  # æœ€æ–°10ä»¶
        print(f"{i}. [{p.get('score', '?')}/10] {p['title'][:60]}")
        print(f"   ğŸ’¡ {p.get('usefulness', '')}")
        print(f"   ğŸ”— {p['link']}\n")


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        cmd = sys.argv[1]
        if cmd == "--scout":
            scout_papers()
        elif cmd == "--notify":
            send_morning_report()
        elif cmd == "--daily":
            scout_papers()
            send_morning_report()
        elif cmd == "--daemon":
            run_scheduler()
        elif cmd == "--stock":
            show_stock()
        else:
            print("Usage: arxiv_scout.py [--scout|--notify|--daily|--daemon|--stock]")
    else:
        # å˜ç™º: å·¡å›â†’é€šçŸ¥
        scout_papers()
        send_morning_report()
